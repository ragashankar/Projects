---
title: "ford_challenge"
author: "Raghavendran Shankar"
date: "11 March 2017"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Loading libraries and datasets

```{r Loading libraries}
library(class) # For knn classification
library(caret) # For confusion matrix
library(rpart) # For decision tree
library(rpart.plot) # For visualizing decision tree

ford_train <- read.csv('fordTrain.csv')
ford_test <- read.csv('fordTest.csv')
```

## Data Preprocessing 

Data is preprocessed i.e to check for missing values and replace it with mean or median; converting the response variable to categorical data; scaling the dataset to calculate the correct eulidean distance
```{r preprocessing, echo=TRUE}
# Converting the response variable from numeric to categorical data
ford_train$IsAlert <- factor(x = ford_train$IsAlert,levels = c(0,1),labels = c(0,1))

# Removing the first two unused columns and scaling the dataset
ford_train <- ford_train[-c(1,2)]
ford_test <- ford_test[-c(1,2)]

ford_train[-c(1,9,27,29)] <- scale(ford_train[-c(1,9,27,29)])
ford_test[-c(1,9,27,29)] <- scale(ford_test[-c(1,9,27,29)])

ford_train[c(9,27,29)] <- NULL
ford_test[c(9,27,29)] <- NULL
```

## Logistic Regression

Also called as linear classifier that works on binomial data. Prediction on test data is done based on probability values obtained from the training model.
```{r Logistic Regression}
# Fitting the ford_train dataset with logistic regression
logit <- glm(formula = IsAlert ~.,family = binomial,data = ford_train)
logit
# Prediciton on ford_test based on probability values obtained from the ford_train
pred_test <- predict(object = logit,newdata = ford_test[-c(1)],type = 'response')

log_pred = ifelse(pred_test < 0.5,0,1) # Probability values of less than 50% are assigned 0 i.e not alert

ford_test$IsAlert <- log_pred # Assigining the output values of Log regression to the test set
plot(pred_test)
```

## K Nearest Neighbours

Knn is a classication method used when prediciting and assigning the new data point to the class which is closer( by finding the distance) and is more in number. It follows lazy learner algorithm and takes the whole training data to buuld the model and predict based on the whole dataset.
```{r K Nearest Neighbours}
# Fitting the dataset with knn
knn_pred <- knn(train = ford_train[,-1],test = ford_test[,-1],cl = ford_train[,1],k = 5)

# Creating confusion matrix to compare the output of logistic regression with the knn output
cm = confusionMatrix(ford_test$IsAlert,knn_pred) # comparing logistic and knn models
cm
plot(knn_pred)
```

## Decision tree

Decision tree model is based on conditions of the predictors. Unlike Logistic regression or knn, the decision tree does not depend on the distance of data points and hence scaling of dataset is not necessary.
The tree can also be pruned to avoid showing the outlier data points which are smal in number.
```{r Decision tree}
# Data preprocessing without scaling the variables
ford_train <- read.csv('fordTrain.csv')
ford_test <- read.csv('fordTest.csv')

ford_train$IsAlert <- factor(x = ford_train$IsAlert,levels = c(0,1),labels = c(0,1))

ford_train <- ford_train[-c(1,2)]
ford_test <- ford_test[-c(1,2)]
ford_train[c(9,27,29)] <- NULL
ford_test[c(9,27,29)] <- NULL

ford_test$IsAlert <- log_pred # Assigining the output values of Log regression to the test set

# Fitting and predicitng the dataset using Decision tree
dtree <- rpart(formula = IsAlert ~.,data = ford_train)
tree_pred <- predict(object = dtree,newdata = ford_test[-c(1)],type = 'class')
# Creating confusion matrix to compare the output of logistic regression with the decision tree output
cm = confusionMatrix(ford_test$IsAlert,tree_pred)
cm

# Visualizing the tree
plot(dtree)
text(dtree)

# Data pruning and visualizing the tree
pruned <- prune(dtree,cp=dtree$cptable[which.min(dtree$cptable[,"xerror"]),"CP"])
prp(pruned)
```

